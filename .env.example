# MADACE_RUST_PY - Environment Configuration Example
# Copy this file to .env and fill in your values

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Planning/Architecture LLM Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# Which LLM to use for planning/architecture work
# Options: gemini, claude, openai, local
# Recommended: gemini (free tier available)
PLANNING_LLM=gemini

# Model selection (based on provider)
# For Gemini: gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash
# For Claude: claude-sonnet-4.5, claude-sonnet-3.5
# For OpenAI: gpt-4-turbo, gpt-4o, gpt-4o-mini
# For Local: llama3.1:70b, llama3.1:8b, codellama:34b
PLANNING_LLM_MODEL=gemini-2.0-flash-exp

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# API Keys (Get from provider dashboards)
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# Google Gemini API Key
# Get from: https://aistudio.google.com/app/apikey
GEMINI_API_KEY=

# Anthropic Claude API Key (if using Claude)
# Get from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=

# OpenAI API Key (if using OpenAI)
# Get from: https://platform.openai.com/api-keys
# OPENAI_API_KEY=

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Model-Specific Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# Gemini Model (if using Gemini)
GEMINI_MODEL=gemini-2.0-flash-exp

# Claude Model (if using Claude)
# CLAUDE_MODEL=claude-sonnet-4.5

# OpenAI Model (if using OpenAI)
# OPENAI_MODEL=gpt-4-turbo

# Ollama Model (if using local)
# OLLAMA_MODEL=llama3.1:8b
# OLLAMA_HOST=http://localhost:11434

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Implementation Agent Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# Implementation uses local Docker agent (automatic)
# No configuration needed - this is set up automatically
IMPLEMENTATION_AGENT=docker
IMPLEMENTATION_MODEL=auto

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Project Configuration
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# Project root directory (automatically detected)
# PROJECT_ROOT=/path/to/MADACE_RUST_PY

# MADACE workflow status file
# WORKFLOW_STATUS=docs/mam-workflow-status.md

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Setup Instructions
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Run interactive setup script:
#    ./scripts/select-llm.sh
#
# 3. Or manually edit .env with your API keys
#
# 4. Test your configuration:
#    ./scripts/test-llm.sh
#
# 5. Start planning:
#    npm run madace -- pm *plan-project

# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# Documentation
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# For detailed information about LLM selection, see:
# docs/LLM-SELECTION.md
#
# For general project information, see:
# README.md
# USING-MADACE.md
